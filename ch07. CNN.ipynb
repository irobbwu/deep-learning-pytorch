{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b3c065f",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每 张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。即使将隐藏层维度降低到1000，这个 全连接层也将有106 × 103 = 109个参数。想要训练这个模型将不可实现，因为需要有大量的GPU、分布式优 化训练的经验和超乎常人的耐心。\n",
    "\n",
    "有些读者可能会反对这个观点，认为要求百万像素的分辨率可能不是必要的。然而，即使分辨率减小为十 万像素，使用1000个隐藏单元的隐藏层也可能不足以学习到良好的图像特征，在真实的系统中我们仍然需要 数十亿个参数。此外，拟合如此多的参数还需要收集大量的数据。然而，如今人类和机器都能很好地区分猫 和狗:这是因为图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。\n",
    "\n",
    "卷积神经网络(convolutional neural networks，CNN)是机器学习利用自然图像中一些已知结构的创造性方法。\n",
    "\n",
    "1. 首先，多层感知机的输入是二维图像X，其隐藏表示H在数学上是一个矩阵，在代码中表示为二维张量。其中X和H具有相同的形状。为了方便理解，我们可以认为，无论是输入还是隐藏表示都拥有空间结构。\n",
    "\n",
    "   使用$[X]_{i,j}$ 和$[H]_{i,j}$ 分别表示输入图像和隐藏表示中位置$(i,j)$处的像素。为了使每个隐藏神经元都能接收到 每个输入像素的信息，我们将参数从权重矩阵(如同我们先前在多层感知机中所做的那样)替换为四阶权重 张量$W$。假设$U$包含偏置参数，我们可以将全连接层形式化地表示为\n",
    "   \n",
    "    $$\\begin{aligned} H_{i,j} =& U_{i,j} + \\sum_k \\sum_l W_{i,j,k,l} X_{k,l}\\\\\n",
    "    =& U_{i,j} + \\sum_a \\sum_b V_{i,j,a,b} X_{i+a,j+b}\\end{aligned}$$\n",
    "\n",
    "\n",
    "\n",
    "   其中，从W到V的转换只是形式上的转换，因为在这两个四阶张量的元素之间存在一一对应的关系。我们只需 重新索引下标$(k, l)$，使$k = i + a、l = j + b$，由此可得$ [V]_{i,j,a,b} = [W]_{i,j,i+a,j+b} $。索引a和b通过在正偏移和负偏 移之间移动覆盖了整个图像。对于隐藏表示中任意给定位置$(i,j)$处的像素值$H_{i,j}$ ，可以通过在x中以(i, j)为 中心对像素进行加权求和得到，加权使用的权重为$V_{i,j,a,b}$ 。\n",
    "   \n",
    "CNN 应该具有以下性质：\n",
    "1. 平移不变性\n",
    "\n",
    "\n",
    "现在引用上述的第一个原则:平移不变性。这意味着检测对象在输入X中的平移，应该仅导致隐藏表示H中的 平移。也就是说，$V$和$U$实际上不依赖于$(i, j)$的值，即$[V]_{i,j,a,b} = [V]_{a,b}$。并且$U$是一个常数，比如$u$。因此，我 们可以简化H定义为:\n",
    "\n",
    "$$Hi,j = u + \\sum_a \\sum_b V_{a,b} X_{i+a,j+b}$$\n",
    "这就是卷积(convolution)。我们是在使用系数$V_{a,b}$ 对位置$(i, j )$附近的像素$(i + a, j + b)$进行加权得到$H_{i,j}$ 。\n",
    "注意，$V_{a,b}$ 的系数比$V_{i,j,a,b}$ 少很多，因为前者不再依赖于图像中的位置。这就是显著的进步!\n",
    "\n",
    "2. 局部性\n",
    "\n",
    "为了收集用来训练参数$H_{i,j}$ 的相关信息，我们不应偏离 到距$(i, j)$很远的地方。这意味着在$|a| > ∆$或$|b| > ∆$的范围之外，我们可以设置$V_{a,b} = 0$。因此，我们可以 将$H_{i,j}$ 重写为\n",
    "\n",
    "$$H_{i,j}=u+\\sum^\\Delta_{a=-\\Delta}\\sum^\\Delta_{b=-\\Delta}V_{a,b}X_{i+a,j+b}$$\n",
    "\n",
    "这个式子表示的layer就是一个convolutional layer。\n",
    "\n",
    "$V$被称为卷积核(convolution kernel)或者滤波器(filter)，亦或简单地称之 为该卷积层的权重，通常该权重是可学习的参数。\n",
    "\n",
    "\n",
    "## convolution\n",
    "\n",
    "在数学中，两个函数(比如$f,g :\\mathbb R_d → \\mathbb R$)之间的“卷积”被定义为\n",
    "\n",
    "$$ (f*g)(\\pmb x) = \\int f(\\pmb z)g(\\pmb{x-z})d\\pmb z$$\n",
    "\n",
    "离散情况下与卷积核和表达方式一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad8bd6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b08b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    " def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) \n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2f38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07245b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48c10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2cac9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e7ed921",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee637635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c5732e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560da882",
   "metadata": {},
   "source": [
    "## 学习卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "782434f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个二维卷积层，它具有1个输出通道和形状为(1，2)的卷积核\n",
    "conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06db8706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个二维卷积层使用四维输入和输出格式(批量大小、通道、高度、宽度)， \n",
    "# 其中批量大小和通道数都为1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2 # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cea1146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 7.012\n",
      "epoch 4, loss 1.981\n",
      "epoch 6, loss 0.662\n",
      "epoch 8, loss 0.246\n",
      "epoch 10, loss 0.097\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    \n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efa46ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[ 0.9584, -1.0216]]]], requires_grad=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(conv2d.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d775f",
   "metadata": {},
   "source": [
    "## 填充和步幅\n",
    "\n",
    "- 填充\n",
    "![title](attachment/cnn_fill.png)\n",
    "\n",
    "通常，如果我们添加ph 行填充(大约一半在顶部，一半在底部)和pw 列填充(左侧大约一半，右侧一半)，则输出形状将为\n",
    "\n",
    "$$(n_h −k_h +p_h +1)×(n_w −k_w +p_w +1)$$\n",
    "\n",
    "在许多情况下，我们需要设置$p_h = k_h − 1$和$p_w = k_w − 1$，使输入和输出具有相同的高度和宽度。\n",
    "\n",
    "- 步幅\n",
    "![title](attachment/cnn_stride.png)\n",
    "\n",
    "通常，当垂直步幅为$s_h$ 、水平步幅为$s_w$ 时，输出形状为\n",
    "\n",
    "$$⌊(n_h −k_h +p_h +s_h)/s_h⌋×⌊(n_w −k_w +p_w +s_w)/s_w⌋.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d10cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# 为了方便起⻅，我们定义了一个计算卷积层的函数。\n",
    "# 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数 \n",
    "def comp_conv2d(conv2d, X):\n",
    "    # 这里的(1，1)表示批量大小和通道数都是1 \n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # 省略前两个维度:批量大小和通道\n",
    "    return Y.reshape(Y.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31e1e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding 的取值是行列的两边都添加，2x2的矩阵就会变成4x4\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b5015b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "517b3e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7ee4ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7818562",
   "metadata": {},
   "source": [
    "## channel\n",
    "\n",
    "当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相 关运算。假设输入的通道数为$c_i$，那么卷积核的输入通道数也需要为$c_i$。如果卷积核的窗口形状是$k_h ×k_w$，那 么当$c_i = 1$时，我们可以把卷积核看作形状为$k_h × k_w$的二维张量。\n",
    "\n",
    "![title](attachment/cnn_channel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2458dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    # 先遍历“X”和“K”的第0个维度(通道维度)，再把它们加在一起 \n",
    "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8659993a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de1cd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。\n",
    "    # 最后将所有结果都叠加在一起\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "347bd8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f38514b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.stack((K, K + 1, K + 2), 0)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03067ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed613d",
   "metadata": {},
   "source": [
    "- 1x1 卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b916c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h*w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    Y = K@X\n",
    "    return Y.reshape((c_o, h ,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "147b7559",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "594e0699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.5076, -2.2076,  1.1770],\n",
       "         [-1.3156,  3.0593,  0.2826],\n",
       "         [-0.8144, -3.1651,  1.1679]],\n",
       "\n",
       "        [[ 1.0557, -0.5774,  0.1752],\n",
       "         [-0.3695, -0.7572, -0.7477],\n",
       "         [ 0.3195,  0.4934, -0.5823]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e54200b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.5076, -2.2076,  1.1770],\n",
       "         [-1.3156,  3.0593,  0.2826],\n",
       "         [-0.8144, -3.1651,  1.1679]],\n",
       "\n",
       "        [[ 1.0557, -0.5774,  0.1752],\n",
       "         [-0.3695, -0.7572, -0.7477],\n",
       "         [ 0.3195,  0.4934, -0.5823]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "Y2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
