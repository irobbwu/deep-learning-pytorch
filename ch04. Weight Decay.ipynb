{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb4a581",
   "metadata": {},
   "source": [
    "# Weight Decay\n",
    "\n",
    "\n",
    "在训练参数化机器学习模型时，权重衰减(weight decay)是最广泛使用的正则化的技术之一，它通常也被 称为L2正则化。这项技术通过函数与零的距离来衡量函数的复杂度，因为在所有函数f中，函数f = 0(所有 输入都得到值0)在某种意义上是最简单的。\n",
    "\n",
    "一种简单的方法是通过线性函数 $f(x) = w^⊤x$ 中的权重向量的某个范数来度量其复杂性，例如$∥w∥^2$。要保证 权重向量比较小，最常用方法是将其范数作为惩罚项加到最小化损失的问题中。将原来的训练目标最小化训 练标签上的预测损失，调整为最小化预测损失和惩罚项之和。现在，如果我们的权重向量增⻓的太大，我们 的学习算法可能会更集中于最小化权重范数$∥w∥^2$。\n",
    "\n",
    "- 参数更新\n",
    "\n",
    "$$loss = \\mathscr{l}(\\pmb w,b) + \\frac{\\lambda}{2}||\\pmb w||^2$$\n",
    "\n",
    "\n",
    "1. gradient\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\pmb w}(\\mathscr{l}(\\pmb w,b) + \\frac{\\lambda}{2}||\\pmb w||^2) = \\frac{\\partial \\mathscr{l}(\\pmb w,b)}{\\partial \\pmb w} + \\lambda·\\pmb w$$\n",
    "\n",
    "2. update gradient\n",
    "$$\\pmb w_{t+1} = (1-\\eta·\\lambda)\\pmb w_t - \\eta\\frac{\\partial \\mathscr{l}(\\pmb w,b)}{\\partial \\pmb w} $$\n",
    "\n",
    "通常$\\eta·\\lambda <1$, named Weight Decay\n",
    "\n",
    "根据之前章节所讲的，我们根据估计值与观测值之间的差异来更新$\\pmb w$。然而，我们同时也在试图将$\\pmb w$的大小 缩小到零。这就是为什么这种方法有时被称为权重衰减。我们仅考虑惩罚项，优化算法在训练的每一步衰减 权重。与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。较小的λ值对应较少 约束的$\\pmb w$，而较大的$\\lambda$值对$\\pmb w$的约束更大。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
